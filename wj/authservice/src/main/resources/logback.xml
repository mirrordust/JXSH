<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>

    <!-- service name -->
    <property name="SERVICE_NAME" value="auth"/>
    <!-- kafka bootstrap servers -->
    <property name="BOOTSTRAP_SERVERS" value="bootstrap.servers=localhost:19092,localhost:19093,localhost:19094"/>


    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <!-- 过滤掉 TRACE 级别的日志-->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>DEBUG</level>
        </filter>
        <encoder>
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤掉 TRACE 和 DEBUG 级别的日志-->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder>
            <pattern>${FILE_LOG_PATTERN}</pattern>
            <charset>utf-8</charset>
        </encoder>
        <file>log/${SERVICE_NAME}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
            <fileNamePattern>log/${SERVICE_NAME}.log.%i</fileNamePattern>
        </rollingPolicy>
        <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <MaxFileSize>1MB</MaxFileSize>
        </triggeringPolicy>
    </appender>

    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <!-- 只接受 ERROR 级别的日志-->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <!-- LogstashEncoder 可以将输出的双引号转义，并且有预定义的字段 -->
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"service_name":"${SERVICE_NAME}","pid":"${PID:-}"}</customFields>
        </encoder>
        <topic>w-logs</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        <!-- Optional parameter to use a fixed partition -->
        <!-- <partition>0</partition> -->

        <!-- Optional parameter to include log timestamps into the kafka message -->
        <!-- <appendTimestamp>true</appendTimestamp> -->

        <!-- each <producerConfig> translates to regular kafka-client config (format: key=value) -->
        <!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers is the only mandatory producerConfig -->
        <producerConfig>${BOOTSTRAP_SERVERS}</producerConfig>

        <!-- this is the fallback appender if kafka is not available. -->
        <!-- <appender-ref ref="CONSOLE" /> -->
    </appender>

    <!-- 异步传递策略，建议选择异步，不然连接kafka失败，会阻挡服务启动 -->
    <appender name="Async" class="ch.qos.logback.classic.AsyncAppender">
        <appender-ref ref="kafkaAppender"/>
    </appender>

    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="FILE"/>
        <appender-ref ref="Async"/>
    </root>

</configuration>
